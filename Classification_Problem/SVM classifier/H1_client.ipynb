{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "d969a343-8df1-4ee2-877b-93bd51169117",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom snowflake.snowpark.context import get_active_session",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcf94556-4cfc-43ed-9641-dad387e18651",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "session = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d278b00a-f871-4e01-bd0a-ec6613a7943a",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "def fetch_from_table_data(session,table_name):\n    table_data = session.table(table_name)\n    return table_data\n\ndata_h1=fetch_from_table_data(session,\"H1\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3641cd03-e028-40a6-b70e-49aae41d9e34",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "def create_dataframe(data):\n    df=data.to_pandas()\n    return df\n\ndf_H1=create_dataframe(data_h1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f732f55-eb5a-469e-b585-44f99cc8fddb",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "def preprocess(df, label_encoder=None, fit_label_encoder=True, scaler=None, fit_scaler=True):\n    # Define a function to extract data within brackets or handle no bracket data\n    def extract_request_type(text):\n        match = re.search(r'\\((.*?)\\)', text)\n        if match:\n            return match.group(1)\n        else:\n            return \"No specific request type\"\n\n    # Apply the function to the DataFrame\n    df['REQUEST_TYPE'] = df['INFO'].apply(extract_request_type)\n\n    # Additional feature based on interactions of Source and Destination\n    df['SOURCE_DESTINATION'] = df['SOURCE'] + '_' + df['DESTINATION']\n\n    # Encoding categorical variables\n    categorical_columns = ['PROTOCOL', 'REQUEST_TYPE', 'SOURCE_DESTINATION']\n    \n    if label_encoder is None:\n        # Initialize LabelEncoder for each categorical column\n        label_encoder = {column: LabelEncoder() for column in categorical_columns}\n\n    for column in categorical_columns:\n        if fit_label_encoder:\n            df[column] = label_encoder[column].fit_transform(df[column])\n        else:\n            df[column] = label_encoder[column].transform(df[column])\n    \n    # # Encode the target variable\n    # if fit_label_encoder:\n    #     df['TYPE_OF_ATTACK'] = label_encoder['TYPE_OF_ATTACK'].fit_transform(df['TYPE_OF_ATTACK'])\n    # else:\n    #     df['TYPE_OF_ATTACK'] = label_encoder['TYPE_OF_ATTACK'].transform(df['TYPE_OF_ATTACK'])\n\n    df.drop(['NO', 'TYPE', 'INFO', 'SOURCE', 'DESTINATION'], axis=1, inplace=True)\n\n    # Feature scaling\n    if scaler is None:\n        scaler = StandardScaler()\n\n    if fit_scaler:\n        df[df.columns] = scaler.fit_transform(df[df.columns])\n    else:\n        df[df.columns] = scaler.transform(df[df.columns])\n\n    return df, label_encoder, scaler",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8d11335-b1bb-4e37-9021-d589fcfef5bc",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false
   },
   "outputs": [],
   "source": "def data_split_train_test(df):\n    x=df.drop(['TYPE_OF_ATTACK'],axis=1)\n    y=df['TYPE_OF_ATTACK']\n    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n    return x_train,x_test,y_train,y_test\n\nx_train_raw,x_test_raw,y_train_raw,y_test_raw=data_split_train_test(df_H1)\n\n\n#preprocess the data\nprocessed_x_train_df,label_encoder,scaler=preprocess(x_train_raw)\nprocessed_x_test_df, _, _ = preprocess(x_test_raw)\n\n#combine x and y for processing\ntrain_df=pd.concat([processed_x_train_df,y_train_raw],axis=1)\ntest_df=pd.concat([processed_x_test_df,y_test_raw],axis=1)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "187405d2-7353-4af8-b082-cfeb5d66b6c8",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "# prepare data for modelling\n\ndef prepare_data_for_modeling(processed_df):\n    x = processed_df.drop(['TYPE_OF_ATTACK'], axis=1)\n    y = processed_df['TYPE_OF_ATTACK']\n\n    # Encode the output as integer labels\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    return x, y\n\nx1_train, y1_train = prepare_data_for_modeling(train_df)\nx1_test, y1_test = prepare_data_for_modeling(test_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a70d2de-9290-496d-ae47-51b610b159ce",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "# learn to predict each class\nclassifier=OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))\ny_score=classifier.fit(x1_train, y1_train).decision_function(x1_test)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e23e61d3-768e-4056-bcc9-a6876c27e909",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "classes = np.unique(y1_train)\n\n# Binarize the output for ROC computation\ny1_test_bin = label_binarize(y1_test, classes=classes)\n \n#Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(classes)):\n    fpr[i], tpr[i], _ = roc_curve(y1_test_bin[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute macro-average ROC curve and ROC area\nfpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(y1_test_bin.ravel(), y_score.ravel())\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73e0c8ab-716d-4351-bf48-3a5d0ca9776a",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "collapsed": false
   },
   "outputs": [],
   "source": "import os\n\n# Set a temporary directory path for Matplotlib configurations\nos.environ['MPLCONFIGDIR'] = '/tmp/matplotlib_config'\n\nplt.figure()\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]))\nfor i in range(len(classes)):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52ef2522-7362-4211-b179-62db3a784b6d",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "from sklearn.metrics import precision_score,confusion_matrix, recall_score, accuracy_score, f1_score\nimport seaborn as sns\ny_pred_classes = np.argmax(y_score, axis=1)\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y1_test, y_pred_classes)\nprecision = precision_score(y1_test, y_pred_classes, average='weighted')\nrecall = recall_score(y1_test, y_pred_classes, average='weighted')\nf1 = f1_score(y1_test, y_pred_classes, average='weighted')\n\n# Print evaluation metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Compute confusion matrix\nconf_matrix = confusion_matrix(y1_test, y_pred_classes)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y1_test), yticklabels=np.unique(y1_test))\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()",
   "execution_count": null
  }
 ]
}